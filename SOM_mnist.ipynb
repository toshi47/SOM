{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOM_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toshi47/SOM/blob/main/SOM_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_xSstz1jefH",
        "outputId": "01898e12-1508-4c36-9349-abf8c754de1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mlrose\n",
            "  Downloading mlrose-1.3.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlrose) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mlrose) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from mlrose) (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->mlrose) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->mlrose) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->mlrose) (3.1.0)\n",
            "Installing collected packages: mlrose\n",
            "Successfully installed mlrose-1.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sompy\n",
            "  Downloading sompy-0.1.1.tar.gz (2.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sompy) (1.21.6)\n",
            "Building wheels for collected packages: sompy\n",
            "  Building wheel for sompy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sompy: filename=sompy-0.1.1-py2.py3-none-any.whl size=3084 sha256=cadf4c5bbc99bdc02be400c7185ffb0bcd7143f2345c091ddd45020df41dd94c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/93/8c/ff042b6386b17bf7751db3c0ea76db7afb88ded186672f3a3e\n",
            "Successfully built sompy\n",
            "Installing collected packages: sompy\n",
            "Successfully installed sompy-0.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/compmonks/SOMPY.git\n",
            "  Cloning https://github.com/compmonks/SOMPY.git to /tmp/pip-req-build-cfqyimmb\n",
            "  Running command git clone -q https://github.com/compmonks/SOMPY.git /tmp/pip-req-build-cfqyimmb\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from SOMPY==1.0) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from SOMPY==1.0) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.7/dist-packages (from SOMPY==1.0) (1.0.2)\n",
            "Requirement already satisfied: numexpr>=2.5 in /usr/local/lib/python3.7/dist-packages (from SOMPY==1.0) (2.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr>=2.5->SOMPY==1.0) (21.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->SOMPY==1.0) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->SOMPY==1.0) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr>=2.5->SOMPY==1.0) (3.0.9)\n",
            "Building wheels for collected packages: SOMPY\n",
            "  Building wheel for SOMPY (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SOMPY: filename=SOMPY-1.0-py3-none-any.whl size=24355 sha256=06517b5408a39a1a2f5131b77386d030885c9398cabc9e033866854faba4b1e1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1d8w6h7n/wheels/95/17/cc/71d201765eff59c98d4407c80f456e45aa3fbf0bcaa238dd07\n",
            "Successfully built SOMPY\n",
            "Installing collected packages: SOMPY\n",
            "  Attempting uninstall: SOMPY\n",
            "    Found existing installation: sompy 0.1.1\n",
            "    Uninstalling sompy-0.1.1:\n",
            "      Successfully uninstalled sompy-0.1.1\n",
            "Successfully installed SOMPY-1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipdb==0.8.1\n",
            "  Downloading ipdb-0.8.1.zip (20 kB)\n",
            "Requirement already satisfied: ipython>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipdb==0.8.1) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=0.10->ipdb==0.8.1) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=0.10->ipdb==0.8.1) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=0.10->ipdb==0.8.1) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=0.10->ipdb==0.8.1) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=0.10->ipdb==0.8.1) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=0.10->ipdb==0.8.1) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=0.10->ipdb==0.8.1) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=0.10->ipdb==0.8.1) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=0.10->ipdb==0.8.1) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=0.10->ipdb==0.8.1) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=0.10->ipdb==0.8.1) (0.7.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.8.1-py3-none-any.whl size=13223 sha256=c8b73afd7564cbbd52a59d7743ca374eac1fb611bc64bef9da3c78c34a4bdb21\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/b0/61/32e2fd085bb8d7498e44e488428a544d83881fca71df32d629\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.8.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
            "Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import six\n",
        "import sys\n",
        "sys.modules['sklearn.externals.six'] = six\n",
        "!pip install mlrose\n",
        "import mlrose\n",
        "import joblib\n",
        "sys.modules['sklearn.externals.joblib'] = joblib\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "!pip install sompy\n",
        "!pip install git+https://github.com/compmonks/SOMPY.git\n",
        "!pip install ipdb==0.8.1\n",
        "import sompy\n",
        "from sompy.sompy import SOMFactory\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "print(type(train_X[0]))\n",
        "train_X=train_X[:50000]\n",
        "train_X=np.reshape(train_X,(50000,784))\n",
        "print(train_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7Ft8qunkMW8",
        "outputId": "b922ff65-23e4-4217-ecea-f51770125d6c"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(50000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapsize = [5,5]\n",
        "#print(dataset)\n",
        "print(train_X.shape)\n",
        "som = sompy.SOMFactory.build(train_X, mapsize, mask=None, mapshape='planar', lattice='rect', normalization='var', initialization='pca', neighborhood='gaussian', training='batch', name='sompy')  \n",
        "\n",
        "#print(dataset)\n",
        "#print(np.shape(dataset))\n",
        "\n",
        "\n",
        "som.train(n_job=1, train_rough_len=100, train_finetune_len=100)  # verbose='debug' will print more, and verbose=None wont print \n",
        "\n",
        "topographic_error = som.calculate_topographic_error() ## Quality of the map itself, SOM nodes themselves\n",
        "quantization_error = np.mean(som._bmu[1]) ## Quality of the affectation of our observations to each node in the SOM map\n",
        "print('Topographic error = %s; Quantization error = %s' % (topographic_error, quantization_error))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwjmQkpzlmAq",
        "outputId": "29e96095-a416-4e28-dbc0-9e06f9588640"
      },
      "execution_count": 121,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 784)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " Training...\n",
            " pca_linear_initialization took: 6.694000 seconds\n",
            " Rough training...\n",
            " radius_ini: 1.000000 , radius_final: 1.000000, trainlen: 100\n",
            "\n",
            " epoch: 1 ---> elapsed time:  0.484000, quantization error: 22.818487\n",
            "\n",
            " epoch: 2 ---> elapsed time:  0.505000, quantization error: 22.520954\n",
            "\n",
            " epoch: 3 ---> elapsed time:  0.506000, quantization error: 21.685646\n",
            "\n",
            " epoch: 4 ---> elapsed time:  0.513000, quantization error: 21.367987\n",
            "\n",
            " epoch: 5 ---> elapsed time:  0.494000, quantization error: 21.270543\n",
            "\n",
            " epoch: 6 ---> elapsed time:  0.507000, quantization error: 21.221736\n",
            "\n",
            " epoch: 7 ---> elapsed time:  0.501000, quantization error: 21.198402\n",
            "\n",
            " epoch: 8 ---> elapsed time:  0.499000, quantization error: 21.184589\n",
            "\n",
            " epoch: 9 ---> elapsed time:  0.498000, quantization error: 21.174809\n",
            "\n",
            " epoch: 10 ---> elapsed time:  0.509000, quantization error: 21.166532\n",
            "\n",
            " epoch: 11 ---> elapsed time:  0.496000, quantization error: 21.159889\n",
            "\n",
            " epoch: 12 ---> elapsed time:  0.487000, quantization error: 21.153589\n",
            "\n",
            " epoch: 13 ---> elapsed time:  0.492000, quantization error: 21.147394\n",
            "\n",
            " epoch: 14 ---> elapsed time:  0.501000, quantization error: 21.141264\n",
            "\n",
            " epoch: 15 ---> elapsed time:  0.503000, quantization error: 21.134991\n",
            "\n",
            " epoch: 16 ---> elapsed time:  0.494000, quantization error: 21.129321\n",
            "\n",
            " epoch: 17 ---> elapsed time:  0.501000, quantization error: 21.124405\n",
            "\n",
            " epoch: 18 ---> elapsed time:  0.493000, quantization error: 21.119960\n",
            "\n",
            " epoch: 19 ---> elapsed time:  0.494000, quantization error: 21.115932\n",
            "\n",
            " epoch: 20 ---> elapsed time:  0.502000, quantization error: 21.111891\n",
            "\n",
            " epoch: 21 ---> elapsed time:  0.496000, quantization error: 21.108695\n",
            "\n",
            " epoch: 22 ---> elapsed time:  0.502000, quantization error: 21.106068\n",
            "\n",
            " epoch: 23 ---> elapsed time:  0.492000, quantization error: 21.104132\n",
            "\n",
            " epoch: 24 ---> elapsed time:  0.501000, quantization error: 21.102735\n",
            "\n",
            " epoch: 25 ---> elapsed time:  0.500000, quantization error: 21.101637\n",
            "\n",
            " epoch: 26 ---> elapsed time:  0.500000, quantization error: 21.101051\n",
            "\n",
            " epoch: 27 ---> elapsed time:  0.498000, quantization error: 21.100834\n",
            "\n",
            " epoch: 28 ---> elapsed time:  0.498000, quantization error: 21.100715\n",
            "\n",
            " epoch: 29 ---> elapsed time:  0.492000, quantization error: 21.100753\n",
            "\n",
            " epoch: 30 ---> elapsed time:  0.502000, quantization error: 21.100918\n",
            "\n",
            " epoch: 31 ---> elapsed time:  0.492000, quantization error: 21.101156\n",
            "\n",
            " epoch: 32 ---> elapsed time:  0.502000, quantization error: 21.101269\n",
            "\n",
            " epoch: 33 ---> elapsed time:  0.525000, quantization error: 21.101518\n",
            "\n",
            " epoch: 34 ---> elapsed time:  0.969000, quantization error: 21.101779\n",
            "\n",
            " epoch: 35 ---> elapsed time:  0.849000, quantization error: 21.102096\n",
            "\n",
            " epoch: 36 ---> elapsed time:  0.785000, quantization error: 21.102272\n",
            "\n",
            " epoch: 37 ---> elapsed time:  0.513000, quantization error: 21.102350\n",
            "\n",
            " epoch: 38 ---> elapsed time:  0.501000, quantization error: 21.102581\n",
            "\n",
            " epoch: 39 ---> elapsed time:  0.507000, quantization error: 21.102879\n",
            "\n",
            " epoch: 40 ---> elapsed time:  0.411000, quantization error: 21.102884\n",
            "\n",
            " epoch: 41 ---> elapsed time:  0.498000, quantization error: 21.103062\n",
            "\n",
            " epoch: 42 ---> elapsed time:  0.506000, quantization error: 21.103282\n",
            "\n",
            " epoch: 43 ---> elapsed time:  0.499000, quantization error: 21.103364\n",
            "\n",
            " epoch: 44 ---> elapsed time:  0.499000, quantization error: 21.103476\n",
            "\n",
            " epoch: 45 ---> elapsed time:  0.506000, quantization error: 21.103575\n",
            "\n",
            " epoch: 46 ---> elapsed time:  0.496000, quantization error: 21.103657\n",
            "\n",
            " epoch: 47 ---> elapsed time:  0.509000, quantization error: 21.103746\n",
            "\n",
            " epoch: 48 ---> elapsed time:  0.418000, quantization error: 21.103777\n",
            "\n",
            " epoch: 49 ---> elapsed time:  0.500000, quantization error: 21.103747\n",
            "\n",
            " epoch: 50 ---> elapsed time:  0.491000, quantization error: 21.103780\n",
            "\n",
            " epoch: 51 ---> elapsed time:  0.501000, quantization error: 21.103798\n",
            "\n",
            " epoch: 52 ---> elapsed time:  0.493000, quantization error: 21.103822\n",
            "\n",
            " epoch: 53 ---> elapsed time:  0.499000, quantization error: 21.103790\n",
            "\n",
            " epoch: 54 ---> elapsed time:  0.423000, quantization error: 21.103793\n",
            "\n",
            " epoch: 55 ---> elapsed time:  0.492000, quantization error: 21.103789\n",
            "\n",
            " epoch: 56 ---> elapsed time:  0.488000, quantization error: 21.103783\n",
            "\n",
            " epoch: 57 ---> elapsed time:  0.496000, quantization error: 21.103759\n",
            "\n",
            " epoch: 58 ---> elapsed time:  0.493000, quantization error: 21.103758\n",
            "\n",
            " epoch: 59 ---> elapsed time:  0.506000, quantization error: 21.103799\n",
            "\n",
            " epoch: 60 ---> elapsed time:  0.496000, quantization error: 21.103809\n",
            "\n",
            " epoch: 61 ---> elapsed time:  0.512000, quantization error: 21.103819\n",
            "\n",
            " epoch: 62 ---> elapsed time:  0.491000, quantization error: 21.103819\n",
            "\n",
            " epoch: 63 ---> elapsed time:  0.499000, quantization error: 21.103796\n",
            "\n",
            " epoch: 64 ---> elapsed time:  0.489000, quantization error: 21.103805\n",
            "\n",
            " epoch: 65 ---> elapsed time:  0.506000, quantization error: 21.103825\n",
            "\n",
            " epoch: 66 ---> elapsed time:  0.490000, quantization error: 21.103821\n",
            "\n",
            " epoch: 67 ---> elapsed time:  0.505000, quantization error: 21.103804\n",
            "\n",
            " epoch: 68 ---> elapsed time:  0.495000, quantization error: 21.103796\n",
            "\n",
            " epoch: 69 ---> elapsed time:  0.491000, quantization error: 21.103796\n",
            "\n",
            " epoch: 70 ---> elapsed time:  0.507000, quantization error: 21.103796\n",
            "\n",
            " epoch: 71 ---> elapsed time:  0.492000, quantization error: 21.103796\n",
            "\n",
            " epoch: 72 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 73 ---> elapsed time:  0.503000, quantization error: 21.103796\n",
            "\n",
            " epoch: 74 ---> elapsed time:  0.501000, quantization error: 21.103796\n",
            "\n",
            " epoch: 75 ---> elapsed time:  0.485000, quantization error: 21.103796\n",
            "\n",
            " epoch: 76 ---> elapsed time:  0.495000, quantization error: 21.103796\n",
            "\n",
            " epoch: 77 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 78 ---> elapsed time:  0.498000, quantization error: 21.103796\n",
            "\n",
            " epoch: 79 ---> elapsed time:  0.498000, quantization error: 21.103796\n",
            "\n",
            " epoch: 80 ---> elapsed time:  0.500000, quantization error: 21.103796\n",
            "\n",
            " epoch: 81 ---> elapsed time:  0.486000, quantization error: 21.103796\n",
            "\n",
            " epoch: 82 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 83 ---> elapsed time:  0.495000, quantization error: 21.103796\n",
            "\n",
            " epoch: 84 ---> elapsed time:  0.503000, quantization error: 21.103796\n",
            "\n",
            " epoch: 85 ---> elapsed time:  0.490000, quantization error: 21.103796\n",
            "\n",
            " epoch: 86 ---> elapsed time:  0.500000, quantization error: 21.103796\n",
            "\n",
            " epoch: 87 ---> elapsed time:  0.486000, quantization error: 21.103796\n",
            "\n",
            " epoch: 88 ---> elapsed time:  0.503000, quantization error: 21.103796\n",
            "\n",
            " epoch: 89 ---> elapsed time:  0.499000, quantization error: 21.103796\n",
            "\n",
            " epoch: 90 ---> elapsed time:  0.501000, quantization error: 21.103796\n",
            "\n",
            " epoch: 91 ---> elapsed time:  0.498000, quantization error: 21.103796\n",
            "\n",
            " epoch: 92 ---> elapsed time:  0.501000, quantization error: 21.103796\n",
            "\n",
            " epoch: 93 ---> elapsed time:  0.493000, quantization error: 21.103796\n",
            "\n",
            " epoch: 94 ---> elapsed time:  0.504000, quantization error: 21.103796\n",
            "\n",
            " epoch: 95 ---> elapsed time:  0.504000, quantization error: 21.103796\n",
            "\n",
            " epoch: 96 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 97 ---> elapsed time:  0.493000, quantization error: 21.103796\n",
            "\n",
            " epoch: 98 ---> elapsed time:  0.502000, quantization error: 21.103796\n",
            "\n",
            " epoch: 99 ---> elapsed time:  0.497000, quantization error: 21.103796\n",
            "\n",
            " epoch: 100 ---> elapsed time:  0.499000, quantization error: 21.103796\n",
            "\n",
            " Finetune training...\n",
            " radius_ini: 1.000000 , radius_final: 1.000000, trainlen: 100\n",
            "\n",
            " epoch: 1 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 2 ---> elapsed time:  0.502000, quantization error: 21.103796\n",
            "\n",
            " epoch: 3 ---> elapsed time:  0.487000, quantization error: 21.103796\n",
            "\n",
            " epoch: 4 ---> elapsed time:  0.500000, quantization error: 21.103796\n",
            "\n",
            " epoch: 5 ---> elapsed time:  0.418000, quantization error: 21.103796\n",
            "\n",
            " epoch: 6 ---> elapsed time:  0.427000, quantization error: 21.103796\n",
            "\n",
            " epoch: 7 ---> elapsed time:  0.498000, quantization error: 21.103796\n",
            "\n",
            " epoch: 8 ---> elapsed time:  0.501000, quantization error: 21.103796\n",
            "\n",
            " epoch: 9 ---> elapsed time:  0.491000, quantization error: 21.103796\n",
            "\n",
            " epoch: 10 ---> elapsed time:  0.505000, quantization error: 21.103796\n",
            "\n",
            " epoch: 11 ---> elapsed time:  0.497000, quantization error: 21.103796\n",
            "\n",
            " epoch: 12 ---> elapsed time:  0.504000, quantization error: 21.103796\n",
            "\n",
            " epoch: 13 ---> elapsed time:  0.490000, quantization error: 21.103796\n",
            "\n",
            " epoch: 14 ---> elapsed time:  0.513000, quantization error: 21.103796\n",
            "\n",
            " epoch: 15 ---> elapsed time:  0.511000, quantization error: 21.103796\n",
            "\n",
            " epoch: 16 ---> elapsed time:  0.515000, quantization error: 21.103796\n",
            "\n",
            " epoch: 17 ---> elapsed time:  0.495000, quantization error: 21.103796\n",
            "\n",
            " epoch: 18 ---> elapsed time:  0.516000, quantization error: 21.103796\n",
            "\n",
            " epoch: 19 ---> elapsed time:  0.510000, quantization error: 21.103796\n",
            "\n",
            " epoch: 20 ---> elapsed time:  0.507000, quantization error: 21.103796\n",
            "\n",
            " epoch: 21 ---> elapsed time:  0.512000, quantization error: 21.103796\n",
            "\n",
            " epoch: 22 ---> elapsed time:  0.511000, quantization error: 21.103796\n",
            "\n",
            " epoch: 23 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 24 ---> elapsed time:  0.517000, quantization error: 21.103796\n",
            "\n",
            " epoch: 25 ---> elapsed time:  0.493000, quantization error: 21.103796\n",
            "\n",
            " epoch: 26 ---> elapsed time:  0.510000, quantization error: 21.103796\n",
            "\n",
            " epoch: 27 ---> elapsed time:  0.604000, quantization error: 21.103796\n",
            "\n",
            " epoch: 28 ---> elapsed time:  0.590000, quantization error: 21.103796\n",
            "\n",
            " epoch: 29 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 30 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 31 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 32 ---> elapsed time:  0.507000, quantization error: 21.103796\n",
            "\n",
            " epoch: 33 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 34 ---> elapsed time:  0.510000, quantization error: 21.103796\n",
            "\n",
            " epoch: 35 ---> elapsed time:  0.507000, quantization error: 21.103796\n",
            "\n",
            " epoch: 36 ---> elapsed time:  0.505000, quantization error: 21.103796\n",
            "\n",
            " epoch: 37 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 38 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 39 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 40 ---> elapsed time:  0.495000, quantization error: 21.103796\n",
            "\n",
            " epoch: 41 ---> elapsed time:  0.499000, quantization error: 21.103796\n",
            "\n",
            " epoch: 42 ---> elapsed time:  0.497000, quantization error: 21.103796\n",
            "\n",
            " epoch: 43 ---> elapsed time:  0.491000, quantization error: 21.103796\n",
            "\n",
            " epoch: 44 ---> elapsed time:  0.438000, quantization error: 21.103796\n",
            "\n",
            " epoch: 45 ---> elapsed time:  0.492000, quantization error: 21.103796\n",
            "\n",
            " epoch: 46 ---> elapsed time:  0.508000, quantization error: 21.103796\n",
            "\n",
            " epoch: 47 ---> elapsed time:  0.501000, quantization error: 21.103796\n",
            "\n",
            " epoch: 48 ---> elapsed time:  0.507000, quantization error: 21.103796\n",
            "\n",
            " epoch: 49 ---> elapsed time:  0.492000, quantization error: 21.103796\n",
            "\n",
            " epoch: 50 ---> elapsed time:  0.497000, quantization error: 21.103796\n",
            "\n",
            " epoch: 51 ---> elapsed time:  0.499000, quantization error: 21.103796\n",
            "\n",
            " epoch: 52 ---> elapsed time:  0.500000, quantization error: 21.103796\n",
            "\n",
            " epoch: 53 ---> elapsed time:  0.495000, quantization error: 21.103796\n",
            "\n",
            " epoch: 54 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 55 ---> elapsed time:  0.492000, quantization error: 21.103796\n",
            "\n",
            " epoch: 56 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 57 ---> elapsed time:  0.497000, quantization error: 21.103796\n",
            "\n",
            " epoch: 58 ---> elapsed time:  0.495000, quantization error: 21.103796\n",
            "\n",
            " epoch: 59 ---> elapsed time:  0.510000, quantization error: 21.103796\n",
            "\n",
            " epoch: 60 ---> elapsed time:  0.492000, quantization error: 21.103796\n",
            "\n",
            " epoch: 61 ---> elapsed time:  0.495000, quantization error: 21.103796\n",
            "\n",
            " epoch: 62 ---> elapsed time:  0.487000, quantization error: 21.103796\n",
            "\n",
            " epoch: 63 ---> elapsed time:  0.501000, quantization error: 21.103796\n",
            "\n",
            " epoch: 64 ---> elapsed time:  0.496000, quantization error: 21.103796\n",
            "\n",
            " epoch: 65 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 66 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 67 ---> elapsed time:  0.502000, quantization error: 21.103796\n",
            "\n",
            " epoch: 68 ---> elapsed time:  0.498000, quantization error: 21.103796\n",
            "\n",
            " epoch: 69 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 70 ---> elapsed time:  0.498000, quantization error: 21.103796\n",
            "\n",
            " epoch: 71 ---> elapsed time:  0.506000, quantization error: 21.103796\n",
            "\n",
            " epoch: 72 ---> elapsed time:  0.500000, quantization error: 21.103796\n",
            "\n",
            " epoch: 73 ---> elapsed time:  0.513000, quantization error: 21.103796\n",
            "\n",
            " epoch: 74 ---> elapsed time:  0.499000, quantization error: 21.103796\n",
            "\n",
            " epoch: 75 ---> elapsed time:  0.501000, quantization error: 21.103796\n",
            "\n",
            " epoch: 76 ---> elapsed time:  0.502000, quantization error: 21.103796\n",
            "\n",
            " epoch: 77 ---> elapsed time:  0.507000, quantization error: 21.103796\n",
            "\n",
            " epoch: 78 ---> elapsed time:  0.492000, quantization error: 21.103796\n",
            "\n",
            " epoch: 79 ---> elapsed time:  0.512000, quantization error: 21.103796\n",
            "\n",
            " epoch: 80 ---> elapsed time:  0.495000, quantization error: 21.103796\n",
            "\n",
            " epoch: 81 ---> elapsed time:  0.500000, quantization error: 21.103796\n",
            "\n",
            " epoch: 82 ---> elapsed time:  0.504000, quantization error: 21.103796\n",
            "\n",
            " epoch: 83 ---> elapsed time:  0.499000, quantization error: 21.103796\n",
            "\n",
            " epoch: 84 ---> elapsed time:  0.488000, quantization error: 21.103796\n",
            "\n",
            " epoch: 85 ---> elapsed time:  0.509000, quantization error: 21.103796\n",
            "\n",
            " epoch: 86 ---> elapsed time:  0.503000, quantization error: 21.103796\n",
            "\n",
            " epoch: 87 ---> elapsed time:  0.516000, quantization error: 21.103796\n",
            "\n",
            " epoch: 88 ---> elapsed time:  0.497000, quantization error: 21.103796\n",
            "\n",
            " epoch: 89 ---> elapsed time:  0.504000, quantization error: 21.103796\n",
            "\n",
            " epoch: 90 ---> elapsed time:  0.502000, quantization error: 21.103796\n",
            "\n",
            " epoch: 91 ---> elapsed time:  0.512000, quantization error: 21.103796\n",
            "\n",
            " epoch: 92 ---> elapsed time:  0.497000, quantization error: 21.103796\n",
            "\n",
            " epoch: 93 ---> elapsed time:  0.504000, quantization error: 21.103796\n",
            "\n",
            " epoch: 94 ---> elapsed time:  0.491000, quantization error: 21.103796\n",
            "\n",
            " epoch: 95 ---> elapsed time:  0.509000, quantization error: 21.103796\n",
            "\n",
            " epoch: 96 ---> elapsed time:  0.490000, quantization error: 21.103796\n",
            "\n",
            " epoch: 97 ---> elapsed time:  0.502000, quantization error: 21.103796\n",
            "\n",
            " epoch: 98 ---> elapsed time:  0.498000, quantization error: 21.103796\n",
            "\n",
            " epoch: 99 ---> elapsed time:  0.502000, quantization error: 21.103796\n",
            "\n",
            " epoch: 100 ---> elapsed time:  0.500000, quantization error: 21.103796\n",
            "\n",
            " Final quantization error: 21.103796\n",
            " train took: 108.757000 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topographic error = 0.10454; Quantization error = 21.10379648765946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sompy.visualization.mapview import View2D\n",
        "v = sompy.mapview.View2DPacked(20, 20, 'test',text_size=10)  \n",
        "map_labels = som.cluster(n_clusters=10)\n",
        "data_labels = np.array([map_labels[int(k)] for k in som._bmu[0]]) # mapping labels from grid to original data\n",
        "print(data_labels.shape)\n",
        "y=train_y[:50000]\n",
        "(y == data_labels).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjb4mOziry_c",
        "outputId": "6d378476-6ab1-44e6-93ad-518af7b84e0f"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2830"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v.show(som, what='cluster')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "T_m3wFXc4Vet",
        "outputId": "32bf9cc8-8172-47a6-82e3-4aa5def6ca84"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sompy/visualization/mapview.py:163: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  ax = self._fig.add_subplot(1, 1, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 180x180 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJYAAACmCAYAAAA4e3ZPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAD9ElEQVR4nO3bP4gUZxzG8edRCcpZaFBUlBPTmBhrMSAoyYkoiJVYaHGlVoLEFJJCbJJCELuApX+qdEkOQtBGURBBLAQJRAVBE6JymFOw8ZdiRjjQu8Bln9n19vuBgTlm7p13l++9Lwe7rioBvbag3xPA/ERYiCAsRBAWIggLEYSFCMJCBGFJsv3Q9tj/HGPc9rVezelDR1iIGPqwbJ+XNCrpJ9tTtr+xvdX2dduTtu/Y3jHt/nHb923/Y/uB7YO2P5P0g6Qv2jEm+/RyBkdVDf0h6aGksfZ8raRnkvao+cPb2f68UtKIpBeSNrb3rpH0eXs+Lulav1/LoBxDv2K9xyFJE1U1UVVvquo3SbfUhCZJbyRttr2kqp5U1d2+zXSAEda71kva326Dk+22tk3Smqp6KemApMOSntj+xfan/ZzsoCKsxvSPeDySdL6qlk07Rqrqe0mqql+raqeabfCepHPvGWPoEVbjL0mftOcXJO21vcv2QtuLbe+wvc72Ktv7bI9Iei1pSs3W+HaMdbY/6n76g4ewGt9J+rbd9g5I2ifphKS/1axgx9W8VwskHZP0WNJzSdslHWnHuCLprqQ/bT/tdPYDyO1/NEBPsWIhgrAQQViIICxEEBYiCAsRhIUIwkIEYSGCsBBBWIggLEQQFiIICxGEhQjCQgRhIYKwELFotou/b9k08J9bvnjyx86edXLr7c6e1TV/fNC9HI8VCxGEhQjCQgRhIYKwEEFYiCAsRBAWIggLEYSFCMJCBGEhgrAQQViIICxEEBYiCAsRhIUIwkIEYSGCsBBBWIggLEQQFiIICxGEhYhZv2K/4upEV/OYu8uvOnvUlak9/31TD3259AN4/2fAioUIwkIEYSGCsBBBWIggLEQQFiIICxGEhQjCQgRhIYKwEEFYiCAsRBAWIggLEYSFCMJCBGEhgrAQQViIICxEEBYiCAsRhIUIwkIEYSHCVTXjxVcnds98cQjdOHyp31OI+Wp0uXs5HisWIggLEYSFCMJCBGEhgrAQQViIICxEEBYiCAsRhIUIwkIEYSGCsBBBWIggLEQQFiIICxGEhQjCQgRhIYKwEEFYiCAsRBAWIggLEYtmu7jt8t6u5jFnq8c2dPasn/841dmzOjd6pqfDsWIhgrAQQViIICxEEBYiCAsRhIUIwkIEYSGCsBBBWIggLEQQFiIICxGEhQjCQgRhIYKwEEFYiCAsRBAWIggLEYSFCMJCBGEhgrAQMetX7L++eXZOg57ecnROv4f5gxULEYSFCMJCBGEhgrAQQViIICxEEBYiCAsRhIUIwkIEYSGCsBDhqur3HDAPsWIhgrAQQViIICxEEBYiCAsR/wLeFBq7+ZabKwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = sompy.hitmap.HitMapView(20, 20, 'hitmap', text_size=10, show_text=True)\n",
        "h.show(som)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y6W2_JWq9A7i",
        "outputId": "e6ca061b-a520-4026-d45f-e89420c51177"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sompy/visualization/hitmap.py:25: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  ax = self._fig.add_subplot(111)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0],\n",
              "       [ 0,  1,  1],\n",
              "       [ 0,  2,  2],\n",
              "       [ 0,  3,  3],\n",
              "       [ 0,  4,  4],\n",
              "       [ 1,  0,  5],\n",
              "       [ 1,  1,  6],\n",
              "       [ 1,  2,  7],\n",
              "       [ 1,  3,  8],\n",
              "       [ 1,  4,  9],\n",
              "       [ 2,  0, 10],\n",
              "       [ 2,  1, 11],\n",
              "       [ 2,  2, 12],\n",
              "       [ 2,  3, 13],\n",
              "       [ 2,  4, 14],\n",
              "       [ 3,  0, 15],\n",
              "       [ 3,  1, 16],\n",
              "       [ 3,  2, 17],\n",
              "       [ 3,  3, 18],\n",
              "       [ 3,  4, 19],\n",
              "       [ 4,  0, 20],\n",
              "       [ 4,  1, 21],\n",
              "       [ 4,  2, 22],\n",
              "       [ 4,  3, 23],\n",
              "       [ 4,  4, 24]])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    }
  ]
}